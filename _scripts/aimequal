#!/usr/bin/env zsh
set -euo pipefail

# Repo root
REPO_PATH="$(cd "$(dirname "$0")/.." && pwd)"
cd "$REPO_PATH"

# Create .temp directory if it doesn't exist
mkdir -p .temp

# Generate timestamp
TIMESTAMP=$(date +"%Y%m%d%H%M%S")
OUTPUT_FILE=".temp/aimequal.${TIMESTAMP}.txt"

# Prune old aimequal files (keep last 20)
echo "Pruning old aimequal output files..."
cd .temp
if ls aimequal.*.txt 1> /dev/null 2>&1; then
    ls -t aimequal.*.txt | tail -n +21 | xargs rm -f 2>/dev/null || true
    echo "Kept last 20 aimequal output files"
else
    echo "No old aimequal files to prune"
fi
cd ..

echo "Running aimequal tests at $(date)"
echo "Output will be saved to: $OUTPUT_FILE"
echo "=========================================="

# Auto-format code with prettier before running tests
echo "Auto-formatting code with Prettier..."
echo "=========================================="
echo "Auto-formatting started at $(date)" >> "$OUTPUT_FILE"

# Capture initial git status
INITIAL_GIT_STATUS=$(git status --porcelain 2>/dev/null || echo "")

# Run prettier to auto-format all files
set +e  # Temporarily disable exit-on-error
PRETTIER_OUTPUT=$(pnpm prettier --write . 2>&1)
PRETTIER_EXIT_CODE=$?
set -e  # Re-enable exit-on-error

if [[ $PRETTIER_EXIT_CODE -ne 0 ]]; then
    echo "⚠️  Warning: Prettier encountered issues during auto-formatting" | tee -a "$OUTPUT_FILE"
    echo "$PRETTIER_OUTPUT" | tee -a "$OUTPUT_FILE"
    echo "Continuing with tests despite prettier issues..." | tee -a "$OUTPUT_FILE"
else
    # Capture git status after prettier
    FINAL_GIT_STATUS=$(git status --porcelain 2>/dev/null || echo "")
    
    # Compare to find which files were modified
    if [[ "$INITIAL_GIT_STATUS" != "$FINAL_GIT_STATUS" ]]; then
        # Get list of modified files
        MODIFIED_FILES=$(git diff --name-only 2>/dev/null || echo "")
        FILE_COUNT=$(echo "$MODIFIED_FILES" | grep -c "^" || echo "0")
        
        if [[ $FILE_COUNT -gt 0 ]]; then
            echo "ℹ️  Auto-formatted $FILE_COUNT files:" | tee -a "$OUTPUT_FILE"
            echo "$MODIFIED_FILES" | while IFS= read -r file; do
                [[ -n "$file" ]] && echo "   - $file" | tee -a "$OUTPUT_FILE"
            done
            echo "" | tee -a "$OUTPUT_FILE"
        fi
    else
        echo "✅ No files needed formatting" | tee -a "$OUTPUT_FILE"
    fi
fi

echo "=========================================="
echo ""

# Function to run command and capture output
run_test() {
    local test_name="$1"
    local command="$2"
    
    echo "Running: $test_name"
    echo "Command: $command"
    echo "------------------------------------------"
    
    # Run command and capture output
    local output
    local exit_code
    
    # Temporarily disable exit-on-error to capture output from failing commands
    set +e
    
    # Use a different approach for prettier/typecheck to ensure output is shown
    if [[ "$test_name" == *"Prettier"* ]] || [[ "$test_name" == *"TypeScript"* ]]; then
        # For these tools, we want to see the output even on failure
        # Run the command directly with tee to capture and display simultaneously
        set +o pipefail  # Temporarily disable pipefail to use pipe_status
        eval "$command" 2>&1 | tee -a "$OUTPUT_FILE"
        exit_code=${pipestatus[1]}  # In zsh, pipestatus is lowercase and 1-indexed
        set -o pipefail  # Re-enable pipefail
    else
        # For other tests, use the original approach
        output=$(eval "$command" 2>&1)
        exit_code=$?
        # Write output to file and display
        echo "$output" | tee -a "$OUTPUT_FILE"
    fi
    
    # Re-enable exit-on-error
    set -e
    
    # Ensure output is flushed before continuing
    sleep 0.1
    
    # Check for errors based on exit code only (best practice)
    local test_passed=false
    
    # Trust the exit code - this is the Unix way
    if [[ $exit_code -eq 0 ]]; then
        test_passed=true
    fi
    
    if [[ $test_passed == true ]]; then
        echo "✅ $test_name: PASSED" | tee -a "$OUTPUT_FILE"
        echo "------------------------------------------"
    else
        # Write failure to both stderr and the log file to ensure it's seen
        echo "❌ $test_name: FAILED" | tee -a "$OUTPUT_FILE" >&2
        echo "Check $OUTPUT_FILE for details" | tee -a "$OUTPUT_FILE" >&2
        echo "------------------------------------------" >&2
        # Give time for output to be displayed
        sleep 1
        exit 1
    fi
}

# Check if web server is running, start if needed
check_and_start_web_server() {
    # Use a high random port to avoid conflicts
    TEST_PORT=$(( 9000 + RANDOM % 1000 ))
    export TEST_PORT  # Export for use in Playwright tests
    echo "Using port ${TEST_PORT} for E2E tests"
    
    # Check if lsof is available
    if command -v lsof &> /dev/null; then
        if lsof -i :${TEST_PORT} > /dev/null 2>&1; then
            echo "Web server already running on port ${TEST_PORT}"
            WEB_SERVER_PID=""
            return 0
        fi
    fi
    
    echo "Starting web server for E2E tests on port ${TEST_PORT}..."
    cd apps/aimeHarness
    # Use CI mode to avoid prompts and explicit port
    CI=1 BROWSER=none npx expo start --web --port ${TEST_PORT} > /dev/null 2>&1 &
    WEB_SERVER_PID=$!
    cd ../..
    
    # Wait for server to be ready
    echo "Waiting for server to start..."
    for i in {1..30}; do
        # Try to check if server is ready using curl if lsof isn't available
        if command -v lsof &> /dev/null; then
            if lsof -i :${TEST_PORT} > /dev/null 2>&1; then
                echo "Web server started successfully on port ${TEST_PORT}"
                return 0
            fi
        else
            # Fallback to curl or wget check
            if command -v curl &> /dev/null && curl -s http://localhost:${TEST_PORT} > /dev/null 2>&1; then
                echo "Web server started successfully on port ${TEST_PORT} (verified via curl)"
                return 0
            elif command -v wget &> /dev/null && wget -q -O /dev/null http://localhost:${TEST_PORT} 2>&1; then
                echo "Web server started successfully on port ${TEST_PORT} (verified via wget)"
                return 0
            fi
        fi
        sleep 1
    done
    
    echo "Failed to start web server after 30 seconds"
    if [[ -n "${WEB_SERVER_PID:-}" ]]; then
        kill $WEB_SERVER_PID 2>/dev/null || true
    fi
    exit 1
}

# Cleanup function
cleanup() {
    if [[ -n "${WEB_SERVER_PID:-}" ]]; then
        echo "Stopping web server (PID: $WEB_SERVER_PID)..."
        kill $WEB_SERVER_PID 2>/dev/null || true
    fi
    if [[ -n "${METRO_PID:-}" ]]; then
        echo "Stopping Metro bundler (PID: $METRO_PID)..."
        kill $METRO_PID 2>/dev/null || true
    fi
}

# Set up cleanup trap
trap cleanup EXIT

# Run tests sequentially, fail fast
run_test "Unit Tests" "pnpm test"
run_test "Code Quality" "pnpm hygiene"
# Prettier format check removed - auto-formatting is now done before tests
run_test "TypeScript Type Check" "pnpm typecheck"

# Run Playwright E2E tests if they exist (with browser kungfu!)
# Skip E2E tests if SKIP_E2E is set
if [[ -n "${SKIP_E2E:-}" ]]; then
    echo "ℹ️  Skipping E2E tests (SKIP_E2E is set)"
elif [[ -f "apps/aimeHarness/playwright.config.ts" ]]; then
    check_and_start_web_server
    # Detect if running in Docker and adjust headed mode
    if [[ -f /.dockerenv ]] || [[ -n "${DOCKER_CONTAINER:-}" ]]; then
        echo "Running in Docker - using headless mode for E2E tests"
        HEADED_MODE="false"
        SLOW_MO_VALUE="0"
    else
        HEADED_MODE="true"
        SLOW_MO_VALUE="500"
    fi
    # Run smoke test only - single browser navigating all pages  
    # TEST_PORT is already exported from check_and_start_web_server
    run_test "Web E2E Smoke Test (aimeHarness)" "cd apps/aimeHarness && TEST_PORT=${TEST_PORT} HEADED=${HEADED_MODE} SLOW_MO=${SLOW_MO_VALUE} pnpm test:smoke:web"
fi

# Run EatGPT Playwright E2E tests if they exist
# Skip E2E tests if SKIP_E2E is set
if [[ -z "${SKIP_E2E:-}" ]] && [[ -f "apps/eatgpt/playwright.config.ts" ]]; then
    # Check if EatGPT web server is running, start if needed
    if command -v lsof &> /dev/null && lsof -i :8082 > /dev/null 2>&1; then
        echo "EatGPT web server already running on port 8082"
        EATGPT_WEB_SERVER_PID=""
    else
        echo "Starting EatGPT web server for E2E tests on port 8082..."
        cd apps/eatgpt
        # Use CI mode to avoid prompts and explicit port
        CI=1 BROWSER=none npx expo start --web --port 8082 > /dev/null 2>&1 &
        EATGPT_WEB_SERVER_PID=$!
        cd ../..
        
        # Wait for server to be ready
        echo "Waiting for EatGPT server to start..."
        for i in {1..30}; do
            if command -v lsof &> /dev/null; then
                if lsof -i :8082 > /dev/null 2>&1; then
                    echo "EatGPT web server started successfully"
                    break
                fi
            else
                # Fallback to curl check
                if curl -s http://localhost:8082 > /dev/null 2>&1; then
                    echo "EatGPT web server started successfully (verified via curl)"
                    break
                fi
            fi
            sleep 1
        done
        
        # Final check
        SERVER_STARTED=false
        if command -v lsof &> /dev/null; then
            lsof -i :8082 > /dev/null 2>&1 && SERVER_STARTED=true
        else
            curl -s http://localhost:8082 > /dev/null 2>&1 && SERVER_STARTED=true
        fi
        
        if [[ "$SERVER_STARTED" != "true" ]]; then
            echo "Failed to start EatGPT web server"
            [[ -n "${EATGPT_WEB_SERVER_PID:-}" ]] && kill $EATGPT_WEB_SERVER_PID 2>/dev/null || true
        else
            # Detect if running in Docker and adjust headed mode
            if [[ -f /.dockerenv ]] || [[ -n "${DOCKER_CONTAINER:-}" ]]; then
                HEADED_MODE="false"
                SLOW_MO_VALUE="0"
            else
                HEADED_MODE="true"
                SLOW_MO_VALUE="500"
            fi
            # Register PID for cleanup
            trap "cleanup; [[ -n '${EATGPT_WEB_SERVER_PID:-}' ]] && kill $EATGPT_WEB_SERVER_PID 2>/dev/null || true" EXIT
            run_test "Web E2E Test (EatGPT)" "cd apps/eatgpt && HEADED=${HEADED_MODE} SLOW_MO=${SLOW_MO_VALUE} pnpm test:smoke:web"
        fi
    fi
fi

# Run Maestro Mobile tests if Android emulator is available
check_and_run_mobile_tests() {
    # Check if Maestro is installed
    if ! command -v maestro &> /dev/null; then
        echo "⚠️  Maestro not installed, skipping mobile tests"
        return 0
    fi
    
    # Check if Android emulator is running
    if ! adb devices 2>/dev/null | grep -q "emulator"; then
        echo "⚠️  No Android emulator detected, skipping mobile tests"
        echo "   To run mobile tests: Start Android emulator and re-run aimequal"
        return 0
    fi
    
    echo "Android emulator detected, preparing mobile tests..."
    
    # Clean Maestro artifacts before tests
    if [[ -x "apps/aimeHarness/__tests__/e2e/maestro/cleanup-artifacts.sh" ]]; then
        echo "Cleaning Maestro artifacts..."
        apps/aimeHarness/__tests__/e2e/maestro/cleanup-artifacts.sh clean
    fi
    
    # Start Metro bundler using automation script
    if [[ -x "apps/aimeHarness/__tests__/e2e/maestro/start-metro.sh" ]]; then
        apps/aimeHarness/__tests__/e2e/maestro/start-metro.sh start
        METRO_STARTED=true
    else
        # Fallback to inline Metro management
        if lsof -i :8081 > /dev/null 2>&1; then
            echo "Metro bundler already running on port 8081"
            METRO_PID=""
        else
            echo "Starting Metro bundler for mobile tests..."
            cd apps/aimeHarness
            npx expo start --no-dev --minify > /dev/null 2>&1 &
            METRO_PID=$!
            cd ../..
            
            # Wait for Metro to be ready
            echo "Waiting for Metro bundler to start..."
            for i in {1..30}; do
                if lsof -i :8081 > /dev/null 2>&1; then
                    echo "Metro bundler started successfully"
                    break
                fi
                sleep 1
            done
            
            if ! lsof -i :8081 > /dev/null 2>&1; then
                echo "Failed to start Metro bundler"
                [[ -n "$METRO_PID" ]] && kill $METRO_PID 2>/dev/null || true
                return 1
            fi
        fi
    fi
    
    # Run the mobile smoke test with timeout
    set +e  # Don't exit on test failure, we want to clean up
    run_test "Mobile E2E Smoke Test" "cd apps/aimeHarness && timeout 120 pnpm test:smoke:mobile"
    TEST_RESULT=$?
    set -e
    
    # Archive artifacts on failure
    if [[ $TEST_RESULT -ne 0 ]] && [[ -x "apps/aimeHarness/__tests__/e2e/maestro/cleanup-artifacts.sh" ]]; then
        echo "Archiving test artifacts due to failure..."
        apps/aimeHarness/__tests__/e2e/maestro/cleanup-artifacts.sh archive smoke_test
    fi
    
    # Cleanup Metro
    if [[ "${METRO_STARTED:-false}" == "true" ]] && [[ -x "apps/aimeHarness/__tests__/e2e/maestro/start-metro.sh" ]]; then
        apps/aimeHarness/__tests__/e2e/maestro/start-metro.sh stop
    elif [[ -n "${METRO_PID:-}" ]]; then
        echo "Stopping Metro bundler (PID: $METRO_PID)..."
        kill $METRO_PID 2>/dev/null || true
    fi
    
    # Prune old artifacts
    if [[ -x "apps/aimeHarness/__tests__/e2e/maestro/cleanup-artifacts.sh" ]]; then
        apps/aimeHarness/__tests__/e2e/maestro/cleanup-artifacts.sh prune
    fi
    
    return $TEST_RESULT
}

# Run mobile tests if available
if [[ -d "apps/aimeHarness/__tests__/e2e/maestro" ]]; then
    check_and_run_mobile_tests
fi

echo "=========================================="

# Verify all expected tests actually ran by checking the log file
echo "Verifying all tests ran..."
EXPECTED_TESTS=(
    "Unit Tests: PASSED"
    "Code Quality: PASSED"
    "TypeScript Type Check: PASSED"
    "Web E2E Smoke Test (aimeHarness): PASSED"
    "Web E2E Test (EatGPT): PASSED"
)

# Add mobile test to expected list if it was attempted
if grep -q "Mobile E2E Smoke Test" "$OUTPUT_FILE"; then
    EXPECTED_TESTS+=("Mobile E2E Smoke Test: PASSED")
fi

echo "Checking log file for expected tests:"
MISSING_TESTS=()
FOUND_TESTS=()
for test in "${EXPECTED_TESTS[@]}"; do
    if grep -q "✅ $test" "$OUTPUT_FILE"; then
        echo "  ✅ Found: $test"
        FOUND_TESTS+=("$test")
    else
        echo "  ❌ Missing: $test"
        MISSING_TESTS+=("$test")
    fi
done

if [ ${#MISSING_TESTS[@]} -gt 0 ]; then
    echo ""
    echo "❌ ERROR: Not all expected tests ran successfully!"
    echo "Missing or failed tests:"
    for test in "${MISSING_TESTS[@]}"; do
        echo "  - $test"
    done
    echo "Check $OUTPUT_FILE for details"
    exit 1
fi

echo ""
echo "✅ All ${#FOUND_TESTS[@]} expected tests verified in log file"
echo "All tests completed successfully!"
echo "Full output saved to: $OUTPUT_FILE"
