#!/usr/bin/env zsh
set -euo pipefail

# Repo root
REPO_PATH="$(cd "$(dirname "$0")/.." && pwd)"
cd "$REPO_PATH"

# Create .temp directory if it doesn't exist
mkdir -p .temp

# Generate timestamp
TIMESTAMP=$(date +"%Y%m%d%H%M%S")
OUTPUT_FILE=".temp/aimequal.${TIMESTAMP}.txt"

# Prune old aimequal files (keep last 20)
echo "Pruning old aimequal output files..."
cd .temp
if ls aimequal.*.txt 1> /dev/null 2>&1; then
    ls -t aimequal.*.txt | tail -n +21 | xargs rm -f 2>/dev/null || true
    echo "Kept last 20 aimequal output files"
else
    echo "No old aimequal files to prune"
fi
cd ..

echo "Running aimequal tests at $(date)"
echo "Output will be saved to: $OUTPUT_FILE"
echo "=========================================="

# Function to run command and capture output
run_test() {
    local test_name="$1"
    local command="$2"
    
    echo "Running: $test_name"
    echo "Command: $command"
    echo "------------------------------------------"
    
    # Run command and capture output
    local output
    local exit_code
    
    # Temporarily disable exit-on-error to capture output from failing commands
    set +e
    
    # Use a different approach for prettier/typecheck to ensure output is shown
    if [[ "$test_name" == *"Prettier"* ]] || [[ "$test_name" == *"TypeScript"* ]]; then
        # For these tools, we want to see the output even on failure
        # Run the command directly with tee to capture and display simultaneously
        set +o pipefail  # Temporarily disable pipefail to use pipe_status
        eval "$command" 2>&1 | tee -a "$OUTPUT_FILE"
        exit_code=${pipestatus[1]}  # In zsh, pipestatus is lowercase and 1-indexed
        set -o pipefail  # Re-enable pipefail
    else
        # For other tests, use the original approach
        output=$(eval "$command" 2>&1)
        exit_code=$?
        # Write output to file and display
        echo "$output" | tee -a "$OUTPUT_FILE"
    fi
    
    # Re-enable exit-on-error
    set -e
    
    # Ensure output is flushed before continuing
    sleep 0.1
    
    # Check for errors based on test type
    local test_passed=false
    
    # For Prettier and TypeCheck, use exit code
    if [[ "$test_name" == *"Prettier"* ]] || [[ "$test_name" == *"TypeScript"* ]] || [[ "$test_name" == *"E2E"* ]]; then
        if [[ $exit_code -eq 0 ]]; then
            test_passed=true
        fi
    else
        # For other tests, check both exit code AND error patterns
        if [[ $exit_code -eq 0 ]] && ! echo "$output" | grep -q "error\|Error\|ERROR\|✖\|failed\|Failed\|FAILED\|Oops! Something went wrong"; then
            test_passed=true
        fi
    fi
    
    if [[ $test_passed == true ]]; then
        echo "✅ $test_name: PASSED" | tee -a "$OUTPUT_FILE"
        echo "------------------------------------------"
    else
        # Write failure to both stderr and the log file to ensure it's seen
        echo "❌ $test_name: FAILED" | tee -a "$OUTPUT_FILE" >&2
        echo "Check $OUTPUT_FILE for details" | tee -a "$OUTPUT_FILE" >&2
        echo "------------------------------------------" >&2
        # Give time for output to be displayed
        sleep 1
        exit 1
    fi
}

# Check if web server is running, start if needed
check_and_start_web_server() {
    if lsof -i :8081 > /dev/null 2>&1; then
        echo "Web server already running on port 8081"
        WEB_SERVER_PID=""
    else
        echo "Starting web server for E2E tests..."
        cd apps/eatgpt
        BROWSER=none pnpm web > /dev/null 2>&1 &
        WEB_SERVER_PID=$!
        cd ../..
        
        # Wait for server to be ready
        echo "Waiting for server to start..."
        for i in {1..30}; do
            if lsof -i :8081 > /dev/null 2>&1; then
                echo "Web server started successfully"
                break
            fi
            sleep 1
        done
        
        if ! lsof -i :8081 > /dev/null 2>&1; then
            echo "Failed to start web server"
            exit 1
        fi
    fi
}

# Cleanup function
cleanup() {
    if [[ -n "${WEB_SERVER_PID:-}" ]]; then
        echo "Stopping web server (PID: $WEB_SERVER_PID)..."
        kill $WEB_SERVER_PID 2>/dev/null || true
    fi
}

# Set up cleanup trap
trap cleanup EXIT

# Run tests sequentially, fail fast
run_test "Unit Tests" "pnpm test"
run_test "Code Quality" "pnpm hygiene"
run_test "Prettier Format Check" "pnpm prettier"
run_test "TypeScript Type Check" "pnpm typecheck"

# Run Playwright E2E tests if they exist (with browser kungfu!)
if [[ -f "apps/eatgpt/playwright.config.ts" ]]; then
    check_and_start_web_server
    # Run smoke test only - single browser navigating all pages
    run_test "Web E2E Smoke Test" "cd apps/eatgpt && HEADED=true SLOW_MO=500 pnpm test:smoke:web"
fi

echo "=========================================="

# Verify all expected tests actually ran by checking the log file
echo "Verifying all tests ran..."
EXPECTED_TESTS=(
    "Unit Tests: PASSED"
    "Code Quality: PASSED"
    "Prettier Format Check: PASSED"
    "TypeScript Type Check: PASSED"
    "Web E2E Smoke Test: PASSED"
)

echo "Checking log file for expected tests:"
MISSING_TESTS=()
FOUND_TESTS=()
for test in "${EXPECTED_TESTS[@]}"; do
    if grep -q "✅ $test" "$OUTPUT_FILE"; then
        echo "  ✅ Found: $test"
        FOUND_TESTS+=("$test")
    else
        echo "  ❌ Missing: $test"
        MISSING_TESTS+=("$test")
    fi
done

if [ ${#MISSING_TESTS[@]} -gt 0 ]; then
    echo ""
    echo "❌ ERROR: Not all expected tests ran successfully!"
    echo "Missing or failed tests:"
    for test in "${MISSING_TESTS[@]}"; do
        echo "  - $test"
    done
    echo "Check $OUTPUT_FILE for details"
    exit 1
fi

echo ""
echo "✅ All ${#FOUND_TESTS[@]} expected tests verified in log file"
echo "All tests completed successfully!"
echo "Full output saved to: $OUTPUT_FILE"
